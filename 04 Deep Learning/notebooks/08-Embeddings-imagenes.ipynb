{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4AL0lmumHRSU",
        "1ak9Bk_lHUB0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/08-Embeddings-imagenes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "3U1EFEfauevM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Embeddings de Imágenes</h1>"
      ],
      "metadata": {
        "id": "EqqQ8nLF21H2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta notebook exploraremos dos modelos pre-entrenados de embeddings de imagenes basado en arquitecturas CNN.\n",
        "\n",
        "El primer modelo está basado en [CLIP](https://github.com/openai/CLIP), es una implementación que no necesita Pytorch ni Tensorflow y ya está especializada en la obtención de embeddings ([documentación](https://github.com/minimaxir/imgbeddings)).\n",
        "\n",
        "El segundo modelo es un autoencoder convolucional implementado y entrenado en una de las sesiones anteriores ([notebook](https://github.com/DCDPUAEM/DCDP/blob/main/04%20Deep%20Learning/notebooks/06-Autoencoders.ipynb)). Leeremos el modelo guardado y entrenado de esa ocasión.\n",
        "\n",
        "Con estos embeddings realizaremos dos tareas:\n",
        "\n",
        "* Obtención de vecinos más cercanos.\n",
        "* Extracción de features para una tarea de clasificación."
      ],
      "metadata": {
        "id": "LeB3ahxhT4TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero, instalamos el módulo para el primer modulo."
      ],
      "metadata": {
        "id": "5Y7aAEOarDTl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmlGXumP1gD6"
      },
      "outputs": [],
      "source": [
        "!pip3 install -qq imgbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example"
      ],
      "metadata": {
        "id": "4AL0lmumHRSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtengamos el embedding de una imagen de prueba"
      ],
      "metadata": {
        "id": "p239Y3VnkIVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "sjApgJQb1nkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4-4M4yG8Y38d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imgbeddings import imgbeddings\n",
        "\n",
        "embeddings_model = imgbeddings()"
      ],
      "metadata": {
        "id": "rOhg-PTc1xMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos el embedding de esta imágen y mostramos sus primeras 5 componentes"
      ],
      "metadata": {
        "id": "g3gWkBLCj_dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = embeddings_model.to_embeddings(image)\n",
        "print(embedding[0][:5])\n",
        "print(embedding[0].shape)"
      ],
      "metadata": {
        "id": "mAN6W-X813Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vecinos más cercanos usando Fashion-MNIST"
      ],
      "metadata": {
        "id": "1ak9Bk_lHUB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una instancia del modelo de embeddings pre-entrando"
      ],
      "metadata": {
        "id": "pX7ZXMVrV8IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imgbeddings import imgbeddings\n",
        "\n",
        "embeddings_model = imgbeddings()"
      ],
      "metadata": {
        "id": "63MtFMQEHZ2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el dataset Fashion-MNIST y re-escalamos"
      ],
      "metadata": {
        "id": "T6ENUYFWWAWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')/255.0\n",
        "x_test = x_test.astype('float32')/255.0"
      ],
      "metadata": {
        "id": "v4bfl9BTf8lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generaremos los embeddings de unas las primeras 10 imágenes. Generar los embeddings de todo el conjunto de entrenamiento o prueba tarda alrededor de 3 horas, por esa razón no lo haremos en esta notebook."
      ],
      "metadata": {
        "id": "2D_fCTnSWWCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "x_sample = x_train[:10]\n",
        "\n",
        "images = [Image.fromarray(x) for x in x_sample]\n",
        "embeddings_sample = embeddings_model.to_embeddings(images)\n",
        "embeddings_sample.shape"
      ],
      "metadata": {
        "id": "jqoAaweMWs_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargaremos los embeddings de todo el conjunto de entrenamiento y prueba, fueron generados previamente de la misma manera mostrada en la celda anterior."
      ],
      "metadata": {
        "id": "Ar3RwoqUZEwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq gdown"
      ],
      "metadata": {
        "id": "R1fvCOG7-HaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- FASHION MNIST ----\n",
        "\n",
        "!gdown 13hmH7mgvGrYYyk0QoWyTGDxv2L3BtfmA # train\n",
        "!gdown 1t5_qj0YryhNKLMuAVoLlALBmvnMT8L3M # test"
      ],
      "metadata": {
        "id": "zfuSPCDwxd-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escojamos el conjunto de entrenamiento o prueba."
      ],
      "metadata": {
        "id": "dpcG7IUnZjoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# X = x_train.copy()\n",
        "# y = y_train.copy()\n",
        "# embeddings = np.load(\"Fmnist_train_imgbeddings.npy\")\n",
        "\n",
        "X = x_test.copy()\n",
        "y = y_test.copy()\n",
        "embeddings = np.load(\"Fmnist_test_imgbeddings.npy\")\n",
        "\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "I2ZvcLJ3yU5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenmos una instancia de [`NearestNeighbors`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) para encontrar de forma **eficiente** los vecinos más cercanos de cualquier embedding. El uso de la métrica `cosine` es muy típico para los embeddings generados por modelos de aprendizaje profundo."
      ],
      "metadata": {
        "id": "u5hVvr5TZnYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "num_neighbors = 3\n",
        "\n",
        "nns = NearestNeighbors(metric='cosine')\n",
        "nns.fit(embeddings)"
      ],
      "metadata": {
        "id": "XjAMOFoEE5ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecciones 4 imágenes al asar del dataset y de cada una, encontremos los 3 vecinos más cercanos. ¿Qué tan buenos son los resultados?"
      ],
      "metadata": {
        "id": "PFgJiZJ5aYCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_to_check = 4\n",
        "\n",
        "idxs_to_check = np.random.choice(list(range(X.shape[0])), num_to_check)\n",
        "\n",
        "distances, NN = nns.kneighbors(embeddings[idxs_to_check,:],\n",
        "                               num_neighbors+1,\n",
        "                               return_distance=True)\n",
        "\n",
        "print(f\"Índices de los vecinos más cercanos:\\n{NN}\")\n",
        "print(f\"Distancia a los vecinos más cercanos:\\n{distances}\")\n",
        "\n",
        "distances = distances[:,1:]\n",
        "NN = NN[:,1:]\n",
        "\n",
        "fig, axs = plt.subplots(num_to_check,num_neighbors+1, figsize=(4*num_neighbors, 4*num_to_check))\n",
        "for k,idx in enumerate(idxs_to_check):\n",
        "    axs[0,k].imshow(X[idx])\n",
        "    axs[0,k].axis('off')\n",
        "    for i,nn_idx in enumerate(NN[k][:num_neighbors]):\n",
        "        axs[i+1,k].imshow(X[nn_idx])\n",
        "        axs[i+1,k].set_title(f\"NN {i+1},\\ndistance:{np.round(distances[k,i],3)}\")\n",
        "        axs[i+1,k].axis('off')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "qeeRgZ5xGm38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features para la clasificación Cats vs Dogs"
      ],
      "metadata": {
        "id": "kR6RHUmPkMxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En una notebook de redes CNN realizamos la tarea de clasificación del dataset *Cats vs Dogs*. Logramos obtener un accuracy del orden de 80%.\n",
        "\n",
        "Usaremos embeddings generados con un autoencoder CNN como features para entrenar un clasificador de Machine Learning *clásico* y ver cómo se compara con el clasificador CNN antes mencionado."
      ],
      "metadata": {
        "id": "-WvhP5SSasSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargemos los modelos ya entrenados del autoencoder CNN"
      ],
      "metadata": {
        "id": "FuR6q20AbSTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1adFS_RZK9b20rTKZlC4fTDuZtOEy9np1    # ENCODER\n",
        "!gdown 1-0Y_aV5XST4GnySszgnIi5AC6xoQOdiG    # DECODER\n",
        "!gdown 1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5    # DATASET CATS VS DOGS, versión reducida"
      ],
      "metadata": {
        "id": "gHf1rFV3obM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "file_name = '/content/cnn_perros_gatos-small.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as myzip:\n",
        "    myzip.extractall()\n",
        "    print('Listo')"
      ],
      "metadata": {
        "id": "AF3i4Dv4pAEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los generadores de imágenes. Observa el `batch_size`."
      ],
      "metadata": {
        "id": "uj9kKTJTg_2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = 'cnn_perros_gatos/train'\n",
        "test_dir = 'cnn_perros_gatos/test'\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(144, 144),\n",
        "        batch_size=1,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(144, 144),\n",
        "        batch_size=1,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "e04THn4TpD2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leemos los modelos encoder y decoder ya entrenados.\n",
        "\n",
        "🔵 ¿Por qué tenemos el WARNING que nos muestra?"
      ],
      "metadata": {
        "id": "jdBvV4pehKo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.saving import load_model\n",
        "\n",
        "CvD_encoder = load_model(\"perros_gatos_encoder_cnn_30epochs.h5\")\n",
        "CvD_decoder = load_model(\"perros_gatos_decoder_cnn_30epochs.h5\")"
      ],
      "metadata": {
        "id": "mVnRx__G2ksU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CvD_encoder.summary()"
      ],
      "metadata": {
        "id": "qBhm4zTOaUTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CvD_decoder.summary()"
      ],
      "metadata": {
        "id": "SznXGl_Vhbwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificación"
      ],
      "metadata": {
        "id": "xsim-pc0VLZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos las features y etiqueta de clase de cada imagen. Esto lo hacemos iterando sobre el generador de imágenes, recuerda que tenemos que detener la iteración manualmente, de otra forma la iteración nunca termina.\n",
        "\n",
        "En este paso es que es importante el `batch_size` definido en el paso anterior.\n",
        "\n",
        "La ejecución dura alrededor de 6 minutos."
      ],
      "metadata": {
        "id": "DhlstXNJhqiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_codes = np.zeros((2000, 512))\n",
        "y_train = np.zeros((2000,))\n",
        "\n",
        "train_tensors = np.zeros((2000, 144, 144, 3))\n",
        "test_tensors = np.zeros((1000, 144, 144, 3))\n",
        "\n",
        "test_codes = np.zeros((1000, 512))\n",
        "y_test = np.zeros((1000,))\n",
        "\n",
        "for k,(xi,yi) in enumerate(train_generator):\n",
        "    if k < 2000:\n",
        "        train_codes[k,:] = CvD_encoder.predict(xi,verbose=0)\n",
        "        y_train[k] = yi[0]\n",
        "        train_tensors[k] = xi[0]\n",
        "    else:\n",
        "        break\n",
        "\n",
        "for k,(xi,yi) in enumerate(test_generator):\n",
        "    if k < 1000:\n",
        "        test_codes[k,:] = CvD_encoder.predict(xi,verbose=0)\n",
        "        y_test[k] = yi[0]\n",
        "        test_tensors[k] = xi[0]\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(train_codes.shape)\n",
        "print(y_train.shape)\n",
        "print(test_codes.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "ToYhiqO3V7m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de este punto, el proceso es el mismo del Machine Learning *clásico*"
      ],
      "metadata": {
        "id": "tUmZMV5gkQ5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el rendimiento de varios algoritmos de Machine Learning"
      ],
      "metadata": {
        "id": "P65eqg5Ykcev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clfs = [SVC(), DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier(), LogisticRegression()]\n",
        "names = [\"SVC\", \"DecisionTreeClassifier\", \"RandomForestClassifier\", \"KNeighborsClassifier\", \"LogisticRegression\"]\n",
        "\n",
        "for clf,name in zip(clfs,names):\n",
        "    clf.fit(train_codes, y_train)\n",
        "    print(f\"{name}: {clf.score(test_codes, y_test)}\")"
      ],
      "metadata": {
        "id": "kFAyqgilc7xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vecinos más cercanos"
      ],
      "metadata": {
        "id": "oPjAUDH-c6Ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos una imagen de prueba"
      ],
      "metadata": {
        "id": "7aK-eNwvpk9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1ofrBZ-E9JO3nAfAYKN0l2B6rsUbXwOVp"
      ],
      "metadata": {
        "id": "ejerOr1qzDlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La preprocesamos"
      ],
      "metadata": {
        "id": "1jqot4qUpm4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import utils\n",
        "\n",
        "img_path = '/content/cat.png'\n",
        "test_image = utils.load_img(img_path, target_size=(144, 144))\n",
        "test_image = utils.img_to_array(test_image)\n",
        "test_image = test_image.reshape(1,144,144,3)\n",
        "test_image /= 255.\n",
        "test_image.shape"
      ],
      "metadata": {
        "id": "vxCxfmyYsRLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La mostramos"
      ],
      "metadata": {
        "id": "1918vV9jpogE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(test_image[0])\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X2hYEdaS0Kxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos el embedding de la imagen de arriba, es decir, obtenemos su codificación latente mediante el codificador pre-entrenado."
      ],
      "metadata": {
        "id": "EhtItNaAYZoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_code = CvD_encoder.predict(test_image)\n",
        "print(test_image_code.shape)\n",
        "test_image_code = test_image_code.reshape(512,) # Hacemos el reshape adecuado\n",
        "test_image_code.shape"
      ],
      "metadata": {
        "id": "iTerF_81VLPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incializamos y entrenamos una instancia de `NearestNeighbors`, una vez más usando la métrica de la similitud coseno."
      ],
      "metadata": {
        "id": "KkZmqFC1Ynpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "num_neighbors = 3\n",
        "nns = NearestNeighbors(metric='cosine')\n",
        "nns.fit(train_codes)"
      ],
      "metadata": {
        "id": "Bs-vAzQXdixd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos las 3 imágenes más similares a la imágen de muestra, junto con sus distancias en el espacio de codificación latente."
      ],
      "metadata": {
        "id": "-T2Rkp-RixnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "distances, NN = nns.kneighbors([test_image_code],\n",
        "                               num_neighbors+1,\n",
        "                               return_distance=True)\n",
        "\n",
        "distances = distances[:,:-1]\n",
        "NN = NN[:,:-1]\n",
        "\n",
        "print(f\"Índices de los vecinos más cercanos:\\n{NN}\")\n",
        "print(f\"Distancia a los vecinos más cercanos:\\n{distances}\")\n",
        "\n",
        "fig, axs = plt.subplots(1,num_neighbors+1, figsize=(4*num_neighbors, 4))\n",
        "axs[0].imshow(test_image[0])\n",
        "axs[0].axis('off')\n",
        "for i,nn_idx in enumerate(NN[0]):\n",
        "    axs[i+1].imshow(train_tensors[nn_idx])\n",
        "    axs[i+1].set_title(f\"NN {i+1},\\ndistance:{round(distances[0,i],2)}\")\n",
        "    axs[i+1].axis('off')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TmKEQcA2dJDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔵 Hay algunas imágenes no tan parecidas, ¿qué factores pueden afectar el rendimiento de esta tarea?"
      ],
      "metadata": {
        "id": "AB1S6dlfjbzu"
      }
    }
  ]
}